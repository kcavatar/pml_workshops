{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.5 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "interpreter": {
      "hash": "2c3bc7dc83c1a9b46d99db8b132f805da16834f865ffb3b3861549cbb2b21749"
    },
    "colab": {
      "name": "1_simple_logistic_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcavatar/pml_workshops/blob/main/1_simple_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDaogA4C3n_9"
      },
      "source": [
        "<a href=\"https://githubtocolab.com/PML-UCF/pml_workshops/blob/main/1_simple_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFWgllsp3oAC"
      },
      "source": [
        "# Simple Logistic Regression\n",
        "\n",
        "Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0ASRU0-3oAE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "from IPython import display\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q2Jeeni3oAH"
      },
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/PML-UCF/pml_workshops/main/data/social_network_ads.csv')\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ytsnx2G3oAK"
      },
      "source": [
        "X = dataset.iloc[:, [2,3]].values\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "Y = dataset.iloc[:, 4].values[:,np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH9x7jXy3oAL"
      },
      "source": [
        "## Binary Logistic Regression\n",
        "\n",
        "Lets consider:\n",
        "\n",
        "$\\hat{y}=p(y=1 \\mid x)$\n",
        "\n",
        "$\\hat{y}$ is the probability that $y=1$, given $x$\n",
        "\n",
        "$1-\\hat{y}=p(y=0 \\mid x)$\n",
        "\n",
        "$\\hat{y}=f(u)$, $u=x w^{T}+b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOkor8gc3oAL"
      },
      "source": [
        "\n",
        "## Loss function\n",
        "$\\operatorname{cost}\\left(\\hat{y}, y\\right)= \\begin{cases}-\\log \\left(\\hat{y}\\right) & \\text { if } y=1 \\\\ -\\log \\left(1-\\hat{y}\\right) \\text { if } y=0\\end{cases}$\n",
        "\n",
        "\n",
        "## Simplified Loss Function\n",
        "$\\operatorname{Cost}\\left(\\hat{y}, y\\right)=-y \\log \\left(\\hat{y}\\right)-(1-y) \\log \\left(1-\\hat{y}\\right)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqwSYrQ43oAM"
      },
      "source": [
        "\n",
        "## Deriving Gradient\n",
        "\n",
        "$z=w_{1} x_{1}+w_{2} x_{2}+b$\n",
        "\n",
        "$\\hat{y}=a=\\sigma(z)$\n",
        "\n",
        "$Loss \\rightarrow L(\\hat{y}, y)$\n",
        "\n",
        "**For $w_{1}$**:\n",
        "\n",
        "$\\frac{\\partial(L)}{\\partial w_{1}}=\\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial(z)}{\\partial w_{1}}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial a}=\\frac{\\partial}{\\partial a}(-y \\log a-(1-y) \\log (1-a))$\n",
        "\n",
        "$=-y\\left(\\frac{1}{a}\\right)-(-1) \\frac{(1-y)}{(1-a)}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial a}=\\left(\\frac{-y}{a}\\right)+\\left(\\frac{1-y}{1-a}\\right)$\n",
        "\n",
        "$\\frac{\\partial a}{\\partial z}=a(1-a)$\n",
        "\n",
        "$\\frac{\\partial z}{\\partial w_{1}}=x_{1}$\n",
        "\n",
        "Then:\n",
        "\n",
        "$\\frac{\\partial(L)}{\\partial w_{1}}=(a-y) \\cdot x_{1}$,\n",
        "\n",
        "and\n",
        "\n",
        "$\\frac{\\partial(L)}{\\partial b}=(a-y)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dncvkr553oAO"
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def d_sigmoid(a):\n",
        "    return a*(1-a)\n",
        "\n",
        "def loss(weights, bias, x, y):\n",
        "    a = forward(weights, bias, x)\n",
        "    return (-1/x.shape[0])*(np.sum((y*np.log(a)) + ((1-y)*(np.log(1-a)))))\n",
        "\n",
        "def d_loss(a, y):\n",
        "    return ((1-y)/(1-a)) - y/a\n",
        "\n",
        "def forward(weights, bias, x):\n",
        "    z = x.dot(weights.T) + bias\n",
        "    a = sigmoid(z)\n",
        "    return a\n",
        "\n",
        "def backward(a, y):\n",
        "    # gradient = d_loss(a, y)*d_sigmoid(a)\n",
        "    gradient = (a - y)\n",
        "    return gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWpAiS2E3oAP"
      },
      "source": [
        "X1, X2 = np.meshgrid(np.arange(start = X[:, 0].min() - 1, stop = X[:, 0].max() + 1, step = 0.1),\n",
        "                    (np.arange(start = X[:, 1].min() -1, stop = X[:, 1].max() + 1, step = 0.1)))\n",
        "def plot(loss, pred, fig, ax1, ax2):\n",
        "    ax1.clear()\n",
        "    ax2.clear()\n",
        "    \n",
        "    ax1.grid(True)\n",
        "    ax1.plot(loss)\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_xlim(0, epochs)\n",
        "\n",
        "    ax2.contourf(X1, X2, pred,\n",
        "                alpha = 0.50, cmap = ListedColormap(('red', 'green')))\n",
        "    ax2.set_xlim(X1.min(), X1.max())\n",
        "    ax2.set_ylim(X2.min(), X2.max())\n",
        "    for i, j in enumerate(np.unique(Y)):\n",
        "        ax2.scatter(X[Y[:,0] == j, 0], X[Y[:,0] == j, 1],\n",
        "                    color = ListedColormap(('red', 'green'))(i), label = j)\n",
        "\n",
        "    ax2.set_xlabel('Age')\n",
        "    ax2.set_ylabel('Estimated Salary')\n",
        "    ax2.legend()\n",
        "    \n",
        "    display.update_display(fig,display_id=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdXdfvGf3oAP"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "**Update model parameters in batches:**\n",
        "\n",
        "$w = w - \\alpha \\cdot \\frac{\\partial(L)}{\\partial w}$\n",
        "\n",
        "and\n",
        "\n",
        "$b = b - \\alpha \\cdot \\frac{\\partial(L)}{\\partial b}$\n",
        "\n",
        "$\\alpha \\rightarrow $ Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARqbUgYV3oAQ"
      },
      "source": [
        "weights = np.random.randn(1, X.shape[1])\n",
        "bias = np.zeros((1, 1))\n",
        "\n",
        "history = {'loss':[]}\n",
        "\n",
        "lr=0.01\n",
        "batch_size=4\n",
        "epochs=20\n",
        "\n",
        "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 4))\n",
        "display.display(fig,display_id=1)\n",
        "\n",
        "for ep in range(epochs):\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "    x_shuffled = X[shuffled_indices]\n",
        "    y_shuffled = Y[shuffled_indices]\n",
        "\n",
        "    # SGD with mini batches\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        xi = x_shuffled[i:i+batch_size]\n",
        "        yi = y_shuffled[i:i+batch_size]\n",
        "\n",
        "        a = forward(weights, bias, xi)\n",
        "        gradient = backward(a, yi)\n",
        "        \n",
        "        weights = weights - lr * (gradient.T @ xi)/batch_size\n",
        "        bias = bias - lr * gradient.mean()\n",
        "\n",
        "    history['loss'] += [loss(weights, bias, x_shuffled, y_shuffled)]\n",
        "\n",
        "    pred = forward(weights, bias, np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
        "    plot(history['loss'], pred, fig, ax1, ax2)\n",
        "    time.sleep(1)\n",
        "display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfDxAE533oAR"
      },
      "source": [
        " _______ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU4-8kZS3oAS"
      },
      "source": [
        "## Using Tensorflow Automatic Differentiation and Gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fan-yrYy3oAS"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def tf_sigmoid(z):\n",
        "    return 1/(1+tf.exp(-z))\n",
        "\n",
        "def tf_forward(w, b, x):\n",
        "    z = x @ w + b\n",
        "    a = tf_sigmoid(z)\n",
        "    return a\n",
        "\n",
        "def tf_loss(a, y):\n",
        "    return (-1/a.shape[0])*(tf.reduce_sum((y*tf.math.log(a)) + ((1-y)*(tf.math.log(1-a)))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBFRsohz3oAS"
      },
      "source": [
        "w = tf.Variable(tf.zeros((X.shape[1], 1)), name='w')\n",
        "b = tf.Variable(tf.random.uniform((1, 1)), name='b')\n",
        "\n",
        "history = {'loss':[]}\n",
        "lr=0.01\n",
        "batch_size=4\n",
        "epochs=20\n",
        "\n",
        "for ep in range(epochs):\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "    x_shuffled = X[shuffled_indices]\n",
        "    y_shuffled = Y[shuffled_indices]\n",
        "\n",
        "    # SGD with mini batches\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        xi = x_shuffled[i:i+batch_size]\n",
        "        yi = y_shuffled[i:i+batch_size]\n",
        "\n",
        "        # Computing gradients with tf.GradientTape\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(w)\n",
        "            tape.watch(b)\n",
        "            a = tf_forward(w, b, xi)\n",
        "            l = tf_loss(a, yi)\n",
        "\n",
        "        [dl_dw, dl_db] = tape.gradient(l, [w, b])\n",
        "        \n",
        "        w = w - lr * dl_dw\n",
        "        b = b - lr * dl_db\n",
        "\n",
        "    history['loss'] += [tf_loss(tf_forward(w, b, x_shuffled), y_shuffled).numpy()]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n07GI7KZ3oAT"
      },
      "source": [
        "pred = tf_forward(w,b,np.array([X1.ravel(), X2.ravel()]).T).numpy().reshape(X1.shape)\n",
        "\n",
        "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 4))\n",
        "plot(history['loss'], pred, fig, ax1, ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ8HXuer3oAU"
      },
      "source": [
        "_________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXp_xWSB3oAU"
      },
      "source": [
        "## Using Tensorflow Model and Layers API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZBNXqhJ3oAU"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential(\n",
        "    Dense(1, activation='sigmoid')\n",
        ")\n",
        "\n",
        "model.compile(optimizer='SGD', loss='binary_crossentropy')\n",
        "history = model.fit(X,Y, batch_size=4, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkiwWMR_3oAV"
      },
      "source": [
        "pred = model.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
        "\n",
        "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10, 4))\n",
        "plot(history.history['loss'], pred, fig, ax1, ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEDfbTxi3oAV"
      },
      "source": [
        " \n",
        " "
      ]
    }
  ]
}